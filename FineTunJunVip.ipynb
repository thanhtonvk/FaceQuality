{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c7ca2-1275-41bd-ad59-fd37f031d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13282969-1411-49aa-9df2-f91836aeaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5dbc59-7f94-43a9-9db9-cf1fab9a7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1vqZl38YdCDijcFFp1cqMH-0FtA2s8KlK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8419407-a608-4646-b951-fb0f18ac44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1vYogKi-qNhYcZ3_ZFFS362rHmORpW0cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a0461-d7b7-4845-bd83-eda22c81ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1hwHxQ58-iMMNE8SEhThjE6wtO8fYGIxu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7bf09-5856-4b3b-9c6a-c20b2e401a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1fg7PdDR5D_K4T6olQlz1ONDJkc5CenhQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01141ee2-f8aa-40d3-ae16-f78808deaeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1SKO-CHgujiqf0ZGCX9veI41ehmfPV6Pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd4e2b-a5cf-4b29-9c98-fbf853d2fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q children_face.zip\n",
    "!unzip -q data_clean.zip\n",
    "!unzip -q data1k.zip\n",
    "!unzip -q data400.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd3bc48-f274-447b-877d-f98a8e636f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('dataset')\n",
    "os.mkdir('dataset/train')\n",
    "os.mkdir('dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de7f91-1884-422d-887b-97a075ce2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_folder = []\n",
    "\n",
    "root = 'data_aligned'\n",
    "for label in os.listdir(root):\n",
    "    try:\n",
    "        list_file = os.listdir(f'{root}/{label}')\n",
    "        if len(list_file)>5:\n",
    "            list_folder.append(f'{root}/{label}')\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "root = \"data_face_jun/train\"\n",
    "for label in os.listdir(root):\n",
    "    try:\n",
    "        list_file = os.listdir(f'{root}/{label}')\n",
    "        if len(list_file)>5:\n",
    "            list_folder.append(f'{root}/{label}')\n",
    "    except:\n",
    "        continue\n",
    "root = 'data_face_jun/test'\n",
    "for label in os.listdir(root):\n",
    "    try:\n",
    "        list_file = os.listdir(f'{root}/{label}')\n",
    "        if len(list_file)>5:\n",
    "            list_folder.append(f'{root}/{label}')\n",
    "    except:\n",
    "        continue\n",
    "root = 'data1k'\n",
    "for label in os.listdir(root):\n",
    "    try:\n",
    "        list_file = os.listdir(f'{root}/{label}')\n",
    "        if len(list_file)>5:\n",
    "            list_folder.append(f'{root}/{label}')\n",
    "    except:\n",
    "        continue\n",
    "root = 'data400'\n",
    "for label in os.listdir(root):\n",
    "    try:\n",
    "        list_file = os.listdir(f'{root}/{label}')\n",
    "        if len(list_file)>5:\n",
    "            list_folder.append(f'{root}/{label}')\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e74c15-f571-4dc3-8802-d7c1d2b88880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(list_folder)\n",
    "train_size = int(0.8*len(list_folder))\n",
    "print(train_size)\n",
    "list_train = list_folder[:train_size]\n",
    "list_test = list_folder[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf36792-40d3-4a15-8b16-b31a4c569fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for path in list_train:\n",
    "    label = path.split('/')[-1]\n",
    "    try:\n",
    "        shutil.copytree(src = path,dst = f'dataset/train/{label}')\n",
    "    except:\n",
    "        continue\n",
    "for path in list_test:\n",
    "    label = path.split('/')[-1]\n",
    "    try:\n",
    "        shutil.copytree(src = path,dst = f'dataset/test/{label}')\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41b6d6c-2143-4ff0-ba12-394fcc92090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install easydict\n",
    "!pip install onnx2torch\n",
    "!pip install tensorboard\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b101d152-5253-4015-8e69-a25e254c3dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path as osp\n",
    "import random\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import models,transforms\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from torch import distributed\n",
    "import math\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "import argparse, os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from easydict import EasyDict as edict\n",
    "import torch\n",
    "import torch\n",
    "from torch import distributed\n",
    "from torch.nn.functional import linear, normalize\n",
    "from onnx2torch import convert\n",
    "import torch\n",
    "import math\n",
    "import logging    \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import distributed\n",
    "rank = 0\n",
    "local_rank = 0\n",
    "world_size = 1\n",
    "try:\n",
    "    distributed.init_process_group(\n",
    "        backend=\"nccl\",\n",
    "        init_method=\"tcp://127.0.0.1:12584\",\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "    )\n",
    "except:\n",
    "    print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c3e98-6b9e-4cf6-be83-89c8957cb897",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = (112,112)\n",
    "mean = [0.5,0.5,0.5]\n",
    "std = [0.5,0.5,0.5]\n",
    "transform = transforms.Compose([\n",
    "          transforms.Resize(resize),\n",
    "          transforms.ToTensor(),\n",
    "          transforms.Normalize(mean,std),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),           # Lật ngang hình ảnh với xác suất 100%\n",
    "    # transforms.ColorJitter(brightness=0.3,            # Thay đổi độ sáng\n",
    "    #                        contrast=0.3),             # Thay đổi độ tương phản\n",
    "    # transforms.GaussianBlur(kernel_size=5,            # Áp dụng làm mờ Gaussian\n",
    "    #                         sigma=(0.1, 2.0)),  \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7837fe-9cb4-435e-b25d-2e1af3d52c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  def __init__(self,file_list):\n",
    "    self.file_list = file_list\n",
    "  def __len__(self):\n",
    "    return len(self.file_list)\n",
    "  def __getitem__(self, index):\n",
    "    img_path = self.file_list[index]\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_transformed = transform(img)\n",
    "    label = get_label(img_path)\n",
    "    return img_transformed,int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691234f8-3cce-4148-9ce5-a755af104e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'dataset/train'\n",
    "classes = os.listdir(directory)\n",
    "def get_label(path):\n",
    "  label_name = path.split('/')[-2]\n",
    "  return classes.index(label_name)\n",
    "\n",
    "\n",
    "X = []\n",
    "for label in os.listdir(directory):\n",
    "  for file_name in os.listdir(f'{directory}/{label}'):\n",
    "    file_path = f'{directory}/{label}/{file_name}'\n",
    "    X.append(file_path)\n",
    "import random\n",
    "random.shuffle(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b83a83-c436-4ecc-9462-72db9c72fad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for path in tqdm(X):\n",
    "    try:\n",
    "        image = Image.open(path).convert('RGB')\n",
    "    except:\n",
    "        os.remove(path)\n",
    "        X.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af94b20-4f09-4462-ad2d-08ffac83d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b806c1e-c58a-4276-82fb-4637cc5afe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle = True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9dd53a-8096-4102-9895-89733626c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders ={\n",
    "    \"train\":train_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683df2f1-c2ce-496c-a804-ec21bccd16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iterator = iter(dataloaders['train'])\n",
    "inputs, labels = next(batch_iterator)\n",
    "print(inputs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6fcfe8-e231-4c5f-885e-c13676130976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = None\n",
    "        self.avg = None\n",
    "        self.sum = None\n",
    "        self.count = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def init_logging(rank, models_root):\n",
    "    if rank == 0:\n",
    "        log_root = logging.getLogger()\n",
    "        log_root.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter(\"Training: %(asctime)s-%(message)s\")\n",
    "        handler_file = logging.FileHandler(os.path.join(models_root, \"training.log\"))\n",
    "        handler_stream = logging.StreamHandler(sys.stdout)\n",
    "        handler_file.setFormatter(formatter)\n",
    "        handler_stream.setFormatter(formatter)\n",
    "        log_root.addHandler(handler_file)\n",
    "        log_root.addHandler(handler_stream)\n",
    "        log_root.info('rank_id: %d' % rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ad0f8-f884-457d-bce9-755caafbcba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PartialFC_V2(torch.nn.Module):\n",
    "\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        margin_loss: Callable,\n",
    "        embedding_size: int,\n",
    "        num_classes: int,\n",
    "        sample_rate: float = 1.0,\n",
    "        fp16: bool = False,\n",
    "    ):\n",
    "\n",
    "        super(PartialFC_V2, self).__init__()\n",
    "        assert (\n",
    "            distributed.is_initialized()\n",
    "        ), \"must initialize distributed before create this\"\n",
    "        self.rank = distributed.get_rank()\n",
    "        self.world_size = distributed.get_world_size()\n",
    "\n",
    "        self.dist_cross_entropy = DistCrossEntropy()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sample_rate: float = sample_rate\n",
    "        self.fp16 = fp16\n",
    "        self.num_local: int = num_classes // self.world_size + int(\n",
    "            self.rank < num_classes % self.world_size\n",
    "        )\n",
    "        self.class_start: int = num_classes // self.world_size * self.rank + min(\n",
    "            self.rank, num_classes % self.world_size\n",
    "        )\n",
    "        self.num_sample: int = int(self.sample_rate * self.num_local)\n",
    "        self.last_batch_size: int = 0\n",
    "\n",
    "        self.is_updated: bool = True\n",
    "        self.init_weight_update: bool = True\n",
    "        self.weight = torch.nn.Parameter(torch.normal(0, 0.01, (self.num_local, embedding_size)))\n",
    "\n",
    "        # margin_loss\n",
    "        if isinstance(margin_loss, Callable):\n",
    "            self.margin_softmax = margin_loss\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    def sample(self, labels, index_positive):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            positive = torch.unique(labels[index_positive], sorted=True).cuda()\n",
    "            if self.num_sample - positive.size(0) >= 0:\n",
    "                perm = torch.rand(size=[self.num_local]).cuda()\n",
    "                perm[positive] = 2.0\n",
    "                index = torch.topk(perm, k=self.num_sample)[1].cuda()\n",
    "                index = index.sort()[0].cuda()\n",
    "            else:\n",
    "                index = positive\n",
    "            self.weight_index = index\n",
    "\n",
    "            labels[index_positive] = torch.searchsorted(index, labels[index_positive])\n",
    "\n",
    "        return self.weight[self.weight_index]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        local_embeddings: torch.Tensor,\n",
    "        local_labels: torch.Tensor,\n",
    "    ):\n",
    "\n",
    "        local_labels.squeeze_()\n",
    "        local_labels = local_labels.long()\n",
    "\n",
    "        batch_size = local_embeddings.size(0)\n",
    "        if self.last_batch_size == 0:\n",
    "            self.last_batch_size = batch_size\n",
    "        assert self.last_batch_size == batch_size, (\n",
    "            f\"last batch size do not equal current batch size: {self.last_batch_size} vs {batch_size}\")\n",
    "\n",
    "        _gather_embeddings = [\n",
    "            torch.zeros((batch_size, self.embedding_size)).cuda()\n",
    "            for _ in range(self.world_size)\n",
    "        ]\n",
    "        _gather_labels = [\n",
    "            torch.zeros(batch_size).long().cuda() for _ in range(self.world_size)\n",
    "        ]\n",
    "        _list_embeddings = AllGather(local_embeddings, *_gather_embeddings)\n",
    "        distributed.all_gather(_gather_labels, local_labels)\n",
    "\n",
    "        embeddings = torch.cat(_list_embeddings)\n",
    "        labels = torch.cat(_gather_labels)\n",
    "\n",
    "        labels = labels.view(-1, 1)\n",
    "        index_positive = (self.class_start <= labels) & (\n",
    "            labels < self.class_start + self.num_local\n",
    "        )\n",
    "        labels[~index_positive] = -1\n",
    "        labels[index_positive] -= self.class_start\n",
    "\n",
    "        if self.sample_rate < 1:\n",
    "            weight = self.sample(labels, index_positive)\n",
    "        else:\n",
    "            weight = self.weight\n",
    "\n",
    "        with torch.cuda.amp.autocast(self.fp16):\n",
    "            norm_embeddings = normalize(embeddings)\n",
    "            norm_weight_activated = normalize(weight)\n",
    "            logits = linear(norm_embeddings, norm_weight_activated)\n",
    "        if self.fp16:\n",
    "            logits = logits.float()\n",
    "        logits = logits.clamp(-1, 1)\n",
    "\n",
    "        logits = self.margin_softmax(logits, labels)\n",
    "        loss = self.dist_cross_entropy(logits, labels)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class DistCrossEntropyFunc(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits: torch.Tensor, label: torch.Tensor):\n",
    "        \"\"\" \"\"\"\n",
    "        batch_size = logits.size(0)\n",
    "        # for numerical stability\n",
    "        max_logits, _ = torch.max(logits, dim=1, keepdim=True)\n",
    "        # local to global\n",
    "        distributed.all_reduce(max_logits, distributed.ReduceOp.MAX)\n",
    "        logits.sub_(max_logits)\n",
    "        logits.exp_()\n",
    "        sum_logits_exp = torch.sum(logits, dim=1, keepdim=True)\n",
    "        # local to global\n",
    "        distributed.all_reduce(sum_logits_exp, distributed.ReduceOp.SUM)\n",
    "        logits.div_(sum_logits_exp)\n",
    "        index = torch.where(label != -1)[0]\n",
    "        # loss\n",
    "        loss = torch.zeros(batch_size, 1, device=logits.device)\n",
    "        loss[index] = logits[index].gather(1, label[index])\n",
    "        distributed.all_reduce(loss, distributed.ReduceOp.SUM)\n",
    "        ctx.save_for_backward(index, logits, label)\n",
    "        return loss.clamp_min_(1e-30).log_().mean() * (-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, loss_gradient):\n",
    "        (\n",
    "            index,\n",
    "            logits,\n",
    "            label,\n",
    "        ) = ctx.saved_tensors\n",
    "        batch_size = logits.size(0)\n",
    "        one_hot = torch.zeros(\n",
    "            size=[index.size(0), logits.size(1)], device=logits.device\n",
    "        )\n",
    "        one_hot.scatter_(1, label[index], 1)\n",
    "        logits[index] -= one_hot\n",
    "        logits.div_(batch_size)\n",
    "        return logits * loss_gradient.item(), None\n",
    "\n",
    "\n",
    "class DistCrossEntropy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistCrossEntropy, self).__init__()\n",
    "\n",
    "    def forward(self, logit_part, label_part):\n",
    "        return DistCrossEntropyFunc.apply(logit_part, label_part)\n",
    "\n",
    "\n",
    "class AllGatherFunc(torch.autograd.Function):\n",
    "    \"\"\"AllGather op with gradient backward\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, *gather_list):\n",
    "        gather_list = list(gather_list)\n",
    "        distributed.all_gather(gather_list, tensor)\n",
    "        return tuple(gather_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grads):\n",
    "        grad_list = list(grads)\n",
    "        rank = distributed.get_rank()\n",
    "        grad_out = grad_list[rank]\n",
    "\n",
    "        dist_ops = [\n",
    "            distributed.reduce(grad_out, rank, distributed.ReduceOp.SUM, async_op=True)\n",
    "            if i == rank\n",
    "            else distributed.reduce(\n",
    "                grad_list[i], i, distributed.ReduceOp.SUM, async_op=True\n",
    "            )\n",
    "            for i in range(distributed.get_world_size())\n",
    "        ]\n",
    "        for _op in dist_ops:\n",
    "            _op.wait()\n",
    "\n",
    "        grad_out *= len(grad_list)  # cooperate with distributed loss function\n",
    "        return (grad_out, *[None for _ in range(len(grad_list))])\n",
    "\n",
    "\n",
    "AllGather = AllGatherFunc.apply\n",
    "\n",
    "class CombinedMarginLoss(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 s,\n",
    "                 m1,\n",
    "                 m2,\n",
    "                 m3,\n",
    "                 interclass_filtering_threshold=0):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        self.m3 = m3\n",
    "        self.interclass_filtering_threshold = interclass_filtering_threshold\n",
    "\n",
    "        # For ArcFace\n",
    "        self.cos_m = math.cos(self.m2)\n",
    "        self.sin_m = math.sin(self.m2)\n",
    "        self.theta = math.cos(math.pi - self.m2)\n",
    "        self.sinmm = math.sin(math.pi - self.m2) * self.m2\n",
    "        self.easy_margin = False\n",
    "\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        index_positive = torch.where(labels != -1)[0]\n",
    "\n",
    "        if self.interclass_filtering_threshold > 0:\n",
    "            with torch.no_grad():\n",
    "                dirty = logits > self.interclass_filtering_threshold\n",
    "                dirty = dirty.float()\n",
    "                mask = torch.ones([index_positive.size(0), logits.size(1)], device=logits.device)\n",
    "                mask.scatter_(1, labels[index_positive], 0)\n",
    "                dirty[index_positive] *= mask\n",
    "                tensor_mul = 1 - dirty\n",
    "            logits = tensor_mul * logits\n",
    "\n",
    "        target_logit = logits[index_positive, labels[index_positive].view(-1)]\n",
    "\n",
    "        if self.m1 == 1.0 and self.m3 == 0.0:\n",
    "            with torch.no_grad():\n",
    "                target_logit.arccos_()\n",
    "                logits.arccos_()\n",
    "                final_target_logit = target_logit + self.m2\n",
    "                logits[index_positive, labels[index_positive].view(-1)] = final_target_logit\n",
    "                logits.cos_()\n",
    "            logits = logits * self.s\n",
    "\n",
    "        elif self.m3 > 0:\n",
    "            final_target_logit = target_logit - self.m3\n",
    "            logits[index_positive, labels[index_positive].view(-1)] = final_target_logit\n",
    "            logits = logits * self.s\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "        return logits\n",
    "\n",
    "class ArcFace(torch.nn.Module):\n",
    "    def __init__(self, s=64.0, margin=0.5):\n",
    "        super(ArcFace, self).__init__()\n",
    "        self.s = s\n",
    "        self.margin = margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.theta = math.cos(math.pi - margin)\n",
    "        self.sinmm = math.sin(math.pi - margin) * margin\n",
    "        self.easy_margin = False\n",
    "\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        index = torch.where(labels != -1)[0]\n",
    "        target_logit = logits[index, labels[index].view(-1)]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_logit.arccos_()\n",
    "            logits.arccos_()\n",
    "            final_target_logit = target_logit + self.margin\n",
    "            logits[index, labels[index].view(-1)] = final_target_logit\n",
    "            logits.cos_()\n",
    "        logits = logits * self.s\n",
    "        return logits\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim import SGD\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "class PolynomialLRWarmup(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_iters, total_iters=5, power=1.0, last_epoch=-1, verbose=False):\n",
    "        super().__init__(optimizer, last_epoch=last_epoch, verbose=verbose)\n",
    "        self.total_iters = total_iters\n",
    "        self.power = power\n",
    "        self.warmup_iters = warmup_iters\n",
    "\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0 or self.last_epoch > self.total_iters:\n",
    "            return [group[\"lr\"] for group in self.optimizer.param_groups]\n",
    "\n",
    "        if self.last_epoch <= self.warmup_iters:\n",
    "            return [base_lr * self.last_epoch / self.warmup_iters for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            l = self.last_epoch\n",
    "            w = self.warmup_iters\n",
    "            t = self.total_iters\n",
    "            decay_factor = ((1.0 - (l - w) / (t - w)) / (1.0 - (l - 1 - w) / (t - w))) ** self.power\n",
    "        return [group[\"lr\"] * decay_factor for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "\n",
    "        if self.last_epoch <= self.warmup_iters:\n",
    "            return [\n",
    "                base_lr * self.last_epoch / self.warmup_iters for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [\n",
    "                (\n",
    "                    base_lr * (1.0 - (min(self.total_iters, self.last_epoch) - self.warmup_iters) / (self.total_iters - self.warmup_iters)) ** self.power\n",
    "                )\n",
    "                for base_lr in self.base_lrs\n",
    "            ]\n",
    "\n",
    "\n",
    "class CallBackLogging(object):\n",
    "    def __init__(self, frequent, total_step, batch_size, start_step=0,writer=None):\n",
    "        self.frequent: int = frequent\n",
    "        self.rank: int = distributed.get_rank()\n",
    "        self.world_size: int = distributed.get_world_size()\n",
    "        self.time_start = time.time()\n",
    "        self.total_step: int = total_step\n",
    "        self.start_step: int = start_step\n",
    "        self.batch_size: int = batch_size\n",
    "        self.writer = writer\n",
    "\n",
    "        self.init = False\n",
    "        self.tic = 0\n",
    "\n",
    "    def __call__(self,\n",
    "                 global_step: int,\n",
    "                 loss: AverageMeter,\n",
    "                 epoch: int,\n",
    "                 fp16: bool,\n",
    "                 learning_rate: float,\n",
    "                 grad_scaler: torch.cuda.amp.GradScaler):\n",
    "        if self.rank == 0 and global_step > 0 and global_step % self.frequent == 0:\n",
    "            if self.init:\n",
    "                try:\n",
    "                    speed: float = self.frequent * self.batch_size / (time.time() - self.tic)\n",
    "                    speed_total = speed * self.world_size\n",
    "                except ZeroDivisionError:\n",
    "                    speed_total = float('inf')\n",
    "\n",
    "                #time_now = (time.time() - self.time_start) / 3600\n",
    "                #time_total = time_now / ((global_step + 1) / self.total_step)\n",
    "                #time_for_end = time_total - time_now\n",
    "                time_now = time.time()\n",
    "                time_sec = int(time_now - self.time_start)\n",
    "                time_sec_avg = time_sec / (global_step - self.start_step + 1)\n",
    "                eta_sec = time_sec_avg * (self.total_step - global_step - 1)\n",
    "                time_for_end = eta_sec/3600\n",
    "                if self.writer is not None:\n",
    "                    self.writer.add_scalar('time_for_end', time_for_end, global_step)\n",
    "                    self.writer.add_scalar('learning_rate', learning_rate, global_step)\n",
    "                    self.writer.add_scalar('loss', loss.avg, global_step)\n",
    "                if fp16:\n",
    "                    msg = \"Speed %.2f samples/sec   Loss %.4f   LearningRate %.6f   Epoch: %d   Global Step: %d   \" \\\n",
    "                          \"Fp16 Grad Scale: %2.f   Required: %1.f hours\" % (\n",
    "                              speed_total, loss.avg, learning_rate, epoch, global_step,\n",
    "                              grad_scaler.get_scale(), time_for_end\n",
    "                          )\n",
    "                else:\n",
    "                    msg = \"Speed %.2f samples/sec   Loss %.4f   LearningRate %.6f   Epoch: %d   Global Step: %d   \" \\\n",
    "                          \"Required: %1.f hours\" % (\n",
    "                              speed_total, loss.avg, learning_rate, epoch, global_step, time_for_end\n",
    "                          )\n",
    "                logging.info(msg)\n",
    "                loss.reset()\n",
    "                self.tic = time.time()\n",
    "            else:\n",
    "                self.init = True\n",
    "                self.tic = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c32f4d-d85f-4e05-9b85-4cad19fa0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import argparse, os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def l2_norm(x, axis=1):\n",
    "    \"\"\"l2 norm\"\"\"\n",
    "    norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "    output = x / norm\n",
    "    return output\n",
    "\n",
    "def postprocess(onnx_output):\n",
    "    return l2_norm(onnx_output)\n",
    "def compute_FPR(gt, pred):\n",
    "    FP = ((pred & ~gt)).sum()\n",
    "    TN = (~pred & ~gt).sum()\n",
    "    FPR = (FP / (FP + TN)).item()\n",
    "    return FPR\n",
    "def renorm(image):\n",
    "    image = image.astype(np.float32)\n",
    "    image = (image - image.min()) / (image.max() - image.min() + 1e-4)\n",
    "    image = (255 * image).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "def unnormalize(image, mean, std):\n",
    "    image = ((image * torch.as_tensor(std).reshape(1, image.size(1), 1, 1).to(image.device)) + torch.as_tensor(mean).reshape(1, image.size(1), 1, 1).to(image.device))\n",
    "    return image\n",
    "metric = torch.nn.CosineSimilarity(dim=-1)\n",
    "def eval_compare(backbone,mode):\n",
    "    backbone.eval()\n",
    "    embedding_1, embedding_2, mapping = list(), list(), dict()\n",
    "    for idx, cls_name in enumerate(tqdm(os.listdir(os.path.join('dataset', mode)))):\n",
    "        front_list, portrait_list = list(), list()\n",
    "        list_path =  os.listdir(os.path.join('dataset', mode, cls_name))\n",
    "        if len(list_path)>1:\n",
    "          fname_1 = list_path[0]\n",
    "          fname_2 = list_path[1]\n",
    "          image_1 = Image.open(os.path.join('dataset', mode, cls_name, fname_1)).convert('RGB')\n",
    "          image_2 = Image.open(os.path.join('dataset', mode, cls_name, fname_2)).convert('RGB')\n",
    "          image_1 = Image.fromarray(renorm(np.array(image_1)))\n",
    "          image_2 = Image.fromarray(renorm(np.array(image_2)))\n",
    "    \n",
    "          tensor_1 = transform(image_1)\n",
    "          tensor_2 = transform(image_2)\n",
    "          inputs = torch.stack([tensor_1, tensor_2], dim=0).to(device)\n",
    "          with torch.no_grad():\n",
    "            embed = backbone(inputs)\n",
    "          embedding_1.append(embed[0])\n",
    "          embedding_2.append(embed[1])\n",
    "          mapping[idx] = cls_name\n",
    "    embedding_1 = torch.stack(embedding_1, dim=0)\n",
    "    embedding_2 = torch.stack(embedding_2, dim=0)\n",
    "    \n",
    "    scores = list()\n",
    "    for e in embedding_1:\n",
    "        scores.append(metric(e.unsqueeze(dim=0), embedding_2))\n",
    "    scores = torch.stack(scores, dim=0)\n",
    "    desc_scores, indices = torch.sort(scores, dim=1, descending=True)\n",
    "    \n",
    "    thresholds = np.arange(0.1, 1.0, 0.025)\n",
    "    acc, max_fpr, avg_fpr = list(), list(), list()\n",
    "    \n",
    "    for thres in thresholds:\n",
    "        FPR, outputs = list(), list()\n",
    "        for q, (indice, score) in enumerate(zip(indices, desc_scores)):\n",
    "            # define query ground-truth like [0, 0, 1, 0, ...]\n",
    "            gt = torch.zeros_like(score)\n",
    "            gt[(indice == q).nonzero().item()] = 1\n",
    "            gt = gt.bool()\n",
    "    \n",
    "            # compute false positive rate\n",
    "            pred = score >= thres\n",
    "            FPR.append(compute_FPR(gt, pred))\n",
    "    \n",
    "            if score[indice == q] >= thres:\n",
    "                outputs.append(True)\n",
    "            else:\n",
    "                outputs.append(False)\n",
    "    \n",
    "        acc.append(np.sum(outputs) / len(outputs))\n",
    "        max_fpr.append(np.amax(FPR))\n",
    "        avg_fpr.append(np.mean(FPR))\n",
    "\n",
    "\n",
    "    # Vẽ biểu đồ\n",
    "    plt.plot(thresholds, acc, label='Recall', marker='o')\n",
    "    plt.plot(thresholds, max_fpr, label='FPR', marker='x')\n",
    "    plt.plot(thresholds, avg_fpr, label='Avg FPR', marker='s')\n",
    "    \n",
    "    # Đặt nhãn và tiêu đề\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Eval face compare {mode}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Hiển thị biểu đồ\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "def get_face_embedding_list(backbone,faces_pil):\n",
    "    tensors = [transform(face_pil) for face_pil in faces_pil]\n",
    "    inputs = torch.stack(tensors, dim=0).to(device)\n",
    "    with torch.no_grad():\n",
    "      embeds = backbone(inputs).cpu().detach().numpy()\n",
    "      return np.array([postprocess(np.expand_dims(embed,0)) for embed  in embeds])\n",
    "def eval_search(backbone,mode):\n",
    "    backbone.eval()\n",
    "    face_ids_map = {}\n",
    "    sources_db = np.empty((0, 512), dtype=np.float32)\n",
    "    targets_db = np.empty((0, 512), dtype=np.float32)\n",
    "    face_ids = []\n",
    "    classes = []\n",
    "    for cls_name in tqdm(os.listdir(f'dataset/{mode}')):\n",
    "        list_file = [f'dataset/{mode}/{cls_name}/{file_name}' for file_name in os.listdir(f'dataset/{mode}/{cls_name}')]\n",
    "        sources = list_file[:int(len(list_file)/2)][:1]\n",
    "        targets = list_file[int(len(list_file)/2):][:1]\n",
    "        if len(sources)>0 and len(targets)>0:\n",
    "            sources_emb = get_face_embedding_list(backbone,[Image.open(source).convert('RGB') for source in sources])\n",
    "            targets_emb =  get_face_embedding_list(backbone,[Image.open(target).convert('RGB') for target in targets])\n",
    "            for source_emb in sources_emb:\n",
    "              for target_emb in targets_emb:\n",
    "                sources_db = np.concatenate([sources_db, source_emb], axis=0)\n",
    "                targets_db = np.concatenate([targets_db,target_emb], axis=0)\n",
    "                classes.append(cls_name)\n",
    "                if cls_name not in face_ids_map:\n",
    "                    face_ids_map[cls_name] = len(face_ids_map)\n",
    "                face_id = face_ids_map[cls_name]\n",
    "                face_ids.append(face_id)\n",
    "    face_ids = np.array(face_ids)\n",
    "    distance = [np.sum(sources_db * targets_db, axis=1)]\n",
    "    label = [face_ids == face_ids]\n",
    "    num_of_sample = len(os.listdir(f'dataset/{mode}'))\n",
    "    for _ in tqdm(range(num_of_sample)):\n",
    "        ids = np.arange(sources_db.shape[0])\n",
    "        np.random.shuffle(ids)\n",
    "        distance.append(np.sum(sources_db[ids] * targets_db, axis=1))\n",
    "        label.append(face_ids[ids] == face_ids)\n",
    "    distance = np.concatenate(distance, axis=0)\n",
    "    label = np.concatenate(label, axis=0)\n",
    "    # cal TRec, FRec, Frec_, TRej\n",
    "    threshs = np.linspace(0., 1.0, 101)\n",
    "    results = np.empty((threshs.shape[0], 2), dtype=np.float32)\n",
    "    for i, thresh in enumerate(threshs):\n",
    "        pos_id = distance > thresh\n",
    "    \n",
    "        TRec = np.sum(pos_id[pos_id] == label[pos_id])\n",
    "        FRec = np.sum(pos_id) - TRec\n",
    "        FRej_Pos = np.sum(label) - TRec\n",
    "        # negative\n",
    "        FRec_Neg = np.sum(pos_id[np.logical_not(label)])\n",
    "        TRej = label[np.logical_not(label)].shape[0] - FRec_Neg\n",
    "    \n",
    "        results[i, 0] = TRec / np.sum(label)  # TRecR\n",
    "        results[i, 1] = TRec / (TRec + FRec + FRec_Neg) if (TRec + FRec + FRec_Neg) > 0 else 0  # Prec\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(threshs, results[:, 0], label=f\"TRecR_0\")\n",
    "    ax.plot(threshs, results[:, 1], label=f\"Prec_0\")\n",
    "    ax.set(xlabel='thresh', ylabel='score (cosine similarity)')\n",
    "    ax.grid()\n",
    "    plt.legend()\n",
    "    plt.title(f'Eval search {mode}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa15bd-0b94-455b-b831-d6630ce0d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mbf_w600k_fine_tune'\n",
    "\n",
    "margin_list = (1.0, 0.0, 0.4)\n",
    "interclass_filtering_threshold=0\n",
    "margin_loss = CombinedMarginLoss(\n",
    "    64,\n",
    "    margin_list[0],\n",
    "    margin_list[1],\n",
    "    margin_list[2],\n",
    "    interclass_filtering_threshold\n",
    ")\n",
    "cfg = edict()\n",
    "cfg.margin_list = (1.0, 0.0, 0.4)\n",
    "cfg.network = model_name\n",
    "cfg.resume = False\n",
    "cfg.output = f'{model_name}/'\n",
    "cfg.embedding_size = 512\n",
    "cfg.sample_rate = 1.0\n",
    "cfg.fp16 = True\n",
    "cfg.weight_decay = 1e-4\n",
    "cfg.batch_size = 128\n",
    "cfg.optimizer = \"adamw\"\n",
    "cfg.lr = 0.00001\n",
    "cfg.verbose = 2000\n",
    "cfg.dali = True\n",
    "cfg.dali_aug = True\n",
    "\n",
    "cfg.num_workers = 6\n",
    "\n",
    "cfg.rec = \"dataset\"\n",
    "cfg.num_classes = len(classes)\n",
    "cfg.num_image = len(X)\n",
    "cfg.num_epoch = 20\n",
    "cfg.warmup_epoch = 2\n",
    "cfg.gradient_acc = 1\n",
    "cfg.frequent = 10\n",
    "cfg.val_targets = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee936f-1d7d-4798-8d40-b83ee567caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_partial_fc = PartialFC_V2(\n",
    "            margin_loss, cfg.embedding_size, cfg.num_classes,\n",
    "            cfg.sample_rate, False)\n",
    "module_partial_fc.train().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e4abf-8f58-4b46-8e6b-f2356fa9a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = convert('w600k_mbf.onnx').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a23e5c7-4bf2-4ad5-85a5-d7229de6fd55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_params_name = [\n",
    "    \"Gemm_96.weight\",\"Gemm_96.bias\",\n",
    "    \"BatchNormalization_97.weight\",\"BatchNormalization_97.bias\"\n",
    "]\n",
    "list_params = []\n",
    "for name, param in backbone.named_parameters():\n",
    "    if name not in list_params_name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "        list_params.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7905e-53ee-4e7f-b4c8-85fe478dfc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(params=[\n",
    "    # {'params': backbone.parameters()},\n",
    "    {'params': backbone.Gemm_96.parameters()},\n",
    "    {'params': backbone.BatchNormalization_97.parameters()},\n",
    "# {'params': backbone.Conv_93.parameters()}\n",
    "],\n",
    "    lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "cfg.total_batch_size = cfg.batch_size * world_size\n",
    "cfg.warmup_step = cfg.num_image // cfg.total_batch_size * cfg.warmup_epoch\n",
    "cfg.total_step = cfg.num_image // cfg.total_batch_size * cfg.num_epoch\n",
    "\n",
    "lr_scheduler = PolynomialLRWarmup(\n",
    "    optimizer=opt,\n",
    "    warmup_iters=cfg.warmup_step,\n",
    "    total_iters=cfg.total_step)\n",
    "loss_am = AverageMeter()\n",
    "amp = torch.cuda.amp.grad_scaler.GradScaler(growth_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa2b64-473a-4da2-8ea0-8d047b367042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_writer = (\n",
    "    SummaryWriter(log_dir=os.path.join(cfg.output, \"tensorboard\"))\n",
    "    if rank == 0\n",
    "    else None\n",
    ")\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "callback_logging = CallBackLogging(\n",
    "    frequent=cfg.frequent,\n",
    "    total_step=cfg.total_step,\n",
    "    batch_size=cfg.batch_size,\n",
    "    start_step = global_step,\n",
    "    writer=summary_writer\n",
    ")\n",
    "best_loss = 1000\n",
    "eval_compare(backbone,'train')\n",
    "eval_search(backbone,'train')\n",
    "eval_compare(backbone,'test')\n",
    "eval_search(backbone,'test')\n",
    "print('-------------------------------------------------------------------------------------')\n",
    "for epoch in range(start_epoch, 20):\n",
    "    running_loss = 0.0\n",
    "    for phase in ['train', 'test']:\n",
    "        if phase =='train':\n",
    "            backbone.train()\n",
    "            module_partial_fc.train()\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                with tqdm(dataloaders[phase],unit=\"batch\") as tepoch:\n",
    "                  for img, local_labels in tepoch:\n",
    "                      tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                      img = img.to(device)\n",
    "                      local_labels = local_labels.to(device)\n",
    "                      global_step += 1\n",
    "                      local_embeddings = backbone(img)\n",
    "                      loss: torch.Tensor = module_partial_fc(local_embeddings, local_labels)\n",
    "                      running_loss += loss.item()\n",
    "                      loss.backward()\n",
    "                        # params_to_clip = list(backbone.Flatten_95.parameters()) + list(backbone.Gemm_96.parameters()) + list(backbone.BatchNormalization_97.parameters()) +list(backbone.PRelu_94.parameters())+list(backbone.Conv_93.parameters())\n",
    "                        # torch.nn.utils.clip_grad_norm_(params_to_clip, max_norm=5.0)\n",
    "                      opt.step()\n",
    "                      opt.zero_grad()\n",
    "                      lr_scheduler.step()\n",
    "                      with torch.no_grad():\n",
    "                          loss_am.update(loss.item(), 1)\n",
    "                          callback_logging(global_step, loss_am, epoch, cfg.fp16, lr_scheduler.get_last_lr()[0], amp)\n",
    "                      tepoch.set_postfix(loss=loss.item())\n",
    "                  epoch_loss = running_loss / len(dataloaders[phase])\n",
    "                  print('{}: Loss: {:.4f}'.format(phase,epoch_loss))\n",
    "                  path_module = os.path.join(cfg.output, f\"{epoch}_model.pt\")\n",
    "                  path_fc = os.path.join(cfg.output, f\"{epoch}_fc_model.pt\")\n",
    "                  torch.save(backbone.state_dict(), path_module)\n",
    "                  torch.save(module_partial_fc.state_dict(), path_fc)\n",
    "        eval_compare(backbone,phase)\n",
    "        eval_search(backbone,phase)\n",
    "        print('-------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef81c4-73af-47d0-9eb7-6cf6d177f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = (\n",
    "    SummaryWriter(log_dir=os.path.join(cfg.output, \"tensorboard\"))\n",
    "    if rank == 0\n",
    "    else None\n",
    ")\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "callback_logging = CallBackLogging(\n",
    "    frequent=cfg.frequent,\n",
    "    total_step=cfg.total_step,\n",
    "    batch_size=cfg.batch_size,\n",
    "    start_step = global_step,\n",
    "    writer=summary_writer\n",
    ")\n",
    "best_loss = 1000\n",
    "eval_compare(backbone,'train')\n",
    "eval_search(backbone,'train')\n",
    "eval_compare(backbone,'test')\n",
    "eval_search(backbone,'test')\n",
    "print('-------------------------------------------------------------------------------------')\n",
    "for epoch in range(start_epoch, 20):\n",
    "    running_loss = 0.0\n",
    "    for phase in ['train', 'test']:\n",
    "        if phase =='train':\n",
    "            backbone.train()\n",
    "            module_partial_fc.train()\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                with tqdm(dataloaders[phase],unit=\"batch\") as tepoch:\n",
    "                  for img, local_labels in tepoch:\n",
    "                      tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                      img = img.to(device)\n",
    "                      local_labels = local_labels.to(device)\n",
    "                      global_step += 1\n",
    "                      local_embeddings = backbone(img)\n",
    "                      loss: torch.Tensor = module_partial_fc(local_embeddings, local_labels)\n",
    "                      running_loss += loss.item()\n",
    "                      loss.backward()\n",
    "                        # params_to_clip = list(backbone.Flatten_95.parameters()) + list(backbone.Gemm_96.parameters()) + list(backbone.BatchNormalization_97.parameters()) +list(backbone.PRelu_94.parameters())+list(backbone.Conv_93.parameters())\n",
    "                        # torch.nn.utils.clip_grad_norm_(params_to_clip, max_norm=5.0)\n",
    "                      opt.step()\n",
    "                      opt.zero_grad()\n",
    "                      lr_scheduler.step()\n",
    "                      with torch.no_grad():\n",
    "                          loss_am.update(loss.item(), 1)\n",
    "                          callback_logging(global_step, loss_am, epoch, cfg.fp16, lr_scheduler.get_last_lr()[0], amp)\n",
    "                      tepoch.set_postfix(loss=loss.item())\n",
    "                  epoch_loss = running_loss / len(dataloaders[phase])\n",
    "                  print('{}: Loss: {:.4f}'.format(phase,epoch_loss))\n",
    "                  path_module = os.path.join(cfg.output, f\"{epoch}_model.pt\")\n",
    "                  path_fc = os.path.join(cfg.output, f\"{epoch}_fc_model.pt\")\n",
    "                  torch.save(backbone.state_dict(), path_module)\n",
    "                  torch.save(module_partial_fc.state_dict(), path_fc)\n",
    "        eval_compare(backbone,phase)\n",
    "        eval_search(backbone,phase)\n",
    "        print('-------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f4baf-c273-47b1-b8f8-e20736f97915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
